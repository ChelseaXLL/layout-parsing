{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GenericParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Applying a similar line of thinking to lexicalor semantic graphs extracted from natural languagedocuments, results in a graph-based ranking modelthat can be applied to a variety of natural languageprocessing applications, where knowledge drawnfrom an entire text is used in making local ranking/selection decisions. Such text-oriented rankingmethods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation (Mihalcea etal., 2004).',\n",
       " 'In this paper, we introduce the TextRank graphbased ranking model for graphs extracted from natural language texts. We investigate and evaluate theapplication of TextRank to two language processingtasks consisting of unsupervised keyword and sen-',\n",
       " \"Graph-based ranking algorithms like Kleinberg'sHITS algorithm (Kleinberg, 1999) or Google'sPageRank (Brin and Page, 1998) have been successfully used in citation analysis, social networks, andthe analysis of the link-structure of the World WideWeb. Arguably, these algorithms can be singled outas key elements of the paradigm-shift triggered inthe field of Web search technology, by providing aWeb page ranking mechanism that relies on the collective knowledge of Web architects rather than individual content analysis of Web pages. In short, agraph-based ranking algorithm is a way of decidingon the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying onlyon local vertex-specific information.\",\n",
       " '1 Introduction',\n",
       " 'In this paper, we introduce TextRank - a graph-basedranking model for text processing, and show how thismodel can be successfully used in natural languageapplications. In particular, we propose two innovative unsupervised methods for keyword and sentenceextraction, and show that the results obtained compare favorably with previously published results onestablished benchmarks.',\n",
       " 'Abstract',\n",
       " 'tence extraction, and show that the results obtainedwith TextRank are competitive with state-of-the-artsystems developed in these areas.',\n",
       " '. The TextRank Model',\n",
       " 'Graph-based ranking algorithms are essentially away of deciding the importance of a vertex withina graph, based on global information recursivelydrawn from the entire graph. The basic idea implemented by a graph-based ranking model is thatof \"voting\" or \"recommendation\". When one vertex links to another one, it is basically casting a votefor that other vertex. The higher the number of votesthat are cast for a vertex, the higher the importanceof the vertex. Moreover, the importance of the vertexcasting the vote determines how important the voteitself is, and this information is also taken into account by the ranking model. Hence, the score associated with a vertex is determined based on the votesthat are cast for it, and the score of the vertices casting these votes.',\n",
       " 'Formally, let G = (V, E) be a directed graph withthe set of vertices V and set of edges F, where F isasubset of V x V. For a given vertex Vj, let In(V;) bethe set of vertices that point to it (predecessors), andlet Out(V;) be the set of vertices that vertex V; pointsto (successors). The score of a vertex V; is defined asfollows (Brin and Page, 1998):',\n",
       " 'S(Vi)=(1-d)+d* our (V5)JTn(V;)',\n",
       " 'where d is a damping factor that can be set between0 and 1, which has the role of integrating into themodel the probability of jumping from a given vertexto another random vertex in the graph. In the contextof Web surfing, this graph-based ranking algorithmimplements the \"random surfer model\", where a userclicks on links at random with a probability d, andjumps to a completely new page with probability 1 -d. The factor d is usually set to 0.85 (Brin and Page,1998), and this is the value we are also using in ourimplementation.',\n",
       " 'However, in our model the graphs are build fromnatural language texts, and may include multiple orpartial links between the units (vertices) that are extracted from text. It may be therefore useful to indicate and incorporate into the model the \"strength\"of the connection between two vertices V; and Vj asa weight w;; added to the corresponding edge thatconnects the two vertices.',\n",
       " '2.1 Undirected Graphs',\n",
       " 'Convergence is achieved when the error rate for any vertexin the graph falls below a given threshold. The error rate of avertex Vj is defined as the difference between the \"real\" score ofthe vertex S(V;) and the score computed at iteration k, S*(V;).Since the real score is not known apriori, this error rate is approximated with the difference between the scores computed attwo successive iterations: $**!(V;) - S*(V;)',\n",
       " 'Figure 1 plots the convergence curves for a randomly generated graph with 250 vertices and 250edges, for a convergence threshold of 0.0001. As theconnectivity of the graph increases (i.e. larger number of edges), convergence is usually achieved afterfewer iterations, and the convergence curves for directed and undirected graphs practically overlap.',\n",
       " '2.2 Weighted Graphs',\n",
       " 'In the context of Web surfing, it is unusual for apage to include multiple or partial links to anotherpage, and hence the original PageRank definition forgraph- based ranking i is assuming unweighted graphs.',\n",
       " \"It is important to notice that although the TextRankapplications described in this paper rely on an algorithm derived from Google's PageRank (Brin andPage, 1998), other graph-based ranking algorithmssuch as e.g. HITS (Kleinberg, 1999) or PositionalFunction (Herings et al., 2001) can be easily integrated into the TextRank model (Mihalcea, 2004).\",\n",
       " 'Although traditionally applied on directed graphs, arecursive graph-based ranking algorithm can be alsoapplied to undirected graphs, in which case the outdegree of a vertex is equal to the in-degree of the vertex. For loosely connected graphs, with the numberof edges proportional with the number of vertices,undirected graphs tend to have more gradual convergence curves.',\n",
       " 'Starting from arbitrary values assigned to eachnode in the graph, the computation iterates until convergence below a given threshold is achieved !. Afterrunning the algorithm, a score is associated with eachvertex, which represents the \"importance\" of the vertex within the graph. Notice that the final valuesobtained after TextRank runs to completion are notaffected by the choice of the initial value, only thenumber of iterations to convergence may be different.',\n",
       " 'Figure 1: Convergence curves for graph-basedranking: directed/undirected, weighted/unweightedgraph, 250 vertices, 250 edges.',\n",
       " 'Consequently, we introduce a new formula forgraph-based ranking that takes into account edgeweights when computing the score associated witha vertex in the graph. Notice that a similar formulacan be defined to integrate vertex weights.',\n",
       " 'Figure | plots the convergence curves for the samesample graph from section 2.1, with random weightsin the interval 0-10 added to the edges. While the final vertex scores (and therefore rankings) differ significantly as compared to their unweighted alternatives, the number of iterations to convergence and theshape of the convergence curves is almost identicalfor weighted and unweighted graphs.',\n",
       " '2.3. Text asa Graph',\n",
       " 'To enable the application of graph-based rankingalgorithms to natural language texts, we have tobuild a graph that represents the text, and interconnects words or other text entities with meaningfulrelations. Depending on the application at hand,text units of various sizes and characteristics can beadded as vertices in the graph, e.g. words, collocations, entire sentences, or others. Similarly, it is theapplication that dictates the type of relations that areused to draw connections between any two such vertices, e.g. lexical or semantic relations, contextualoverlap, etc.',\n",
       " 'Regardless of the type and characteristics of the elements added to the graph, the application of graphbased ranking algorithms to natural language textsconsists of the following main steps:']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tesseract\n",
    "parser =  GenericParser(filename = 'textrank.pdf',\n",
    "                        output_name = 'textrank-sample',\n",
    "                        output_path = '/shared/shawn/layout-parser',\n",
    "                        model_name = 'PubLayNet', \n",
    "                        language = 'eng',\n",
    "                        method = 'tesseract',\n",
    "                        start_page = 0,\n",
    "                        end_page = 2,\n",
    "                        cl = 0.6)\n",
    "\n",
    "parser.parse_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Applying a similar line of thinking to lexical or semantic graphs extracted from natural language documents, results in a graph-based ranking model that can be applied to a variety of natural language processing applications, where knowledge drawn from an entire text is used in making local ranking/selection decisions. Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation (Mihalcea et al., 2004 . In this paper, we introduce the TextRank graphbased ranking model for graphs extracted from natural language texts. We investigate and evaluate the application of TextRank to two language processing tasks consisting of unsupervised keyword and sen- Graph-based ranking algorithms like Kleinberg\\'s HITS algorithm (Kleinberg, 1999) or Google\\'s PageRank (Brin and Page, 1998 ) have been successfully used in citation analysis, social networks, and the analysis of the link-structure of the World Wide Web. Arguably, these algorithms can be singled out as key elements of the paradigm-shift triggered in the field of Web search technology, by providing a Web page ranking mechanism that relies on the collective knowledge of Web architects rather than individual content analysis of Web pages. In short, a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information. Introduction In this paper, we introduce TextRank-a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications. In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks. Abstract tence extraction, and show that the results obtained with TextRank are competitive with state-of-the-art systems developed in these areas. The TextRank Model Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph, based on global information recursively drawn from the entire graph. The basic idea implemented by a graph-based ranking model is that of \"voting\" or \"recommendation\". When one vertex links to another one, it is basically casting a vote for that other vertex. The higher the number of votes that are cast for a vertex, the higher the importance of the vertex. Moreover, the importance of the vertex casting the vote determines how important the vote itself is, and this information is also taken into account by the ranking model. Hence, the score associated with a vertex is determined based on the votes that are cast for it, and the score of the vertices casting these votes. Formally, let $G=(V, E)$ be a directed graph with the set of vertices $V$ and set of edges $E$ , where $E$ is a subset of $V \\\\times V$ . For a given vertex $V_{i}$ , let $\\\\operatorname{In}\\\\left(V_{i}\\\\right)$ be the set of vertices that point to it (predecessors), and let $O u t\\\\left(V_{i}\\\\right)$ be the set of vertices that vertex $V_{i}$ points to (successors). The score of a vertex $V_{i}$ is defined as follows (Brin and Page, 1998 ): $S\\\\left(V_{i}\\\\right)=(1-d)+d * \\\\sum_{j \\\\in I_{n}\\\\left(V_{i}\\\\right)} \\\\frac{1}{\\\\operatorname{Out}\\\\left(V_{j}\\\\right)} S\\\\left(V_{j}\\\\right)$ where $d$ is a damping factor that can be set between 0 and 1, which has the role of integrating into the model the probability of jumping from a given vertex to another random vertex in the graph. In the context of Web surfing, this graph-based ranking algorithm implements the \"random surfer model\", where a user clicks on links at random with a probability $d$ , and jumps to a completely new page with probability $1-$ \\n $d$ . The factor $d$ is usually set to $0.85$ (Brin and Page, 1998 ), and this is the value we are also using in our implementation.',\n",
       "  'page': 1,\n",
       "  'doc': 'textrank.pdf'},\n",
       " {'text': 'However, in our model the graphs are build from natural language texts, and may include multiple or partial links between the units (vertices) that are extracted from text. It may be therefore useful to indicate and incorporate into the model the \"strength\" of the connection between two vertices $V_{i}$ and $V_{j}$ as a weight $w_{i j}$ added to the corresponding edge that connects the two vertices. 2.1 Undirected Graphs 1 Convergence is achieved when the error rate for any vertex in the graph falls below a given threshold. The error rate of a vertex $V_{i}$ is defined as the difference between the \"real\" score of the vertex $S\\\\left(V_{i}\\\\right)$ and the score computed at iteration $k, S^{k}\\\\left(V_{i}\\\\right)$ . Since the real score is not known apriori, this error rate is approximated with the difference between the scores computed at two successive iterations: $S^{k+1}\\\\left(V_{i}\\\\right)-S^{k}\\\\left(V_{i}\\\\right)$ . Figure 1 plots the convergence curves for a randomly generated graph with 250 vertices and 250 edges, for a convergence threshold of $0.0001 .$ As the connectivity of the graph increases (i.e. larger number of edges), convergence is usually achieved after fewer iterations, and the convergence curves for directed and undirected graphs practically overlap. 2.2 $\\\\quad$ Weighted Graphs In the context of Web surfing, it is unusual for a page to include multiple or partial links to another page, and hence the original PageRank definition for graph-based ranking is assuming unweighted graphs. It is important to notice that although the TextRank applications described in this paper rely on an algorithm derived from Google\\'s PageRank (Brin and Page, 1998 ), other graph-based ranking algorithms such as e.g. HITS (Kleinberg, 1999) or Positional Function (Herings et al., 2001) can be easily integrated into the TextRank model (Mihalcea, 2004 ). Although traditionally applied on directed graphs, a recursive graph-based ranking algorithm can be also applied to undirected graphs, in which case the outdegree of a vertex is equal to the in-degree of the vertex. For loosely connected graphs, with the number of edges proportional with the number of vertices, undirected graphs tend to have more gradual convergence curves. Starting from arbitrary values assigned to each node in the graph, the computation iterates until convergence below a given threshold is achieved ${ }^{1}$ . After running the algorithm, a score is associated with each vertex, which represents the \"importance\" of the vertex within the graph. Notice that the final values obtained after TextRank runs to completion are not affected by the choice of the initial value, only the number of iterations to convergence may be different. Figure 1: Convergence curves for graph-based ranking: directed/undirected, weighted/unweighted graph, 250 vertices, 250 edges. Consequently, we introduce a new formula for graph-based ranking that takes into account edge weights when computing the score associated with a vertex in the graph. Notice that a similar formula can be defined to integrate vertex weights. Figure 1 plots the convergence curves for the same sample graph from section $2.1$ , with random weights in the interval $0-10$ added to the edges. While the final vertex scores (and therefore rankings) differ significantly as compared to their unweighted alternatives, the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs. 2.3 Text as a Graph To enable the application of graph-based ranking algorithms to natural language texts, we have to build a graph that represents the text, and interconnects words or other text entities with meaningful relations. Depending on the application at hand, text units of various sizes and characteristics can be added as vertices in the graph, e.g. words, collocations, entire sentences, or others. Similarly, it is the application that dictates the type of relations that are used to draw connections between any two such vertices, e.g. lexical or semantic relations, contextual overlap, etc. Regardless of the type and characteristics of the elements added to the graph, the application of graphbased ranking algorithms to natural language texts consists of the following main steps:',\n",
       "  'page': 2,\n",
       "  'doc': 'textrank.pdf'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mathpix\n",
    "parser =  GenericParser.GenericParser(filename = 'textrank.pdf',\n",
    "                        output_name = 'textrank-sample',\n",
    "                        output_path = '/shared/shawn/layout-parser',\n",
    "                        model_name = 'PubLayNet', \n",
    "                        language = 'eng',\n",
    "                        method = 'mathpix',\n",
    "                        start_page = 0,\n",
    "                        end_page = 2,\n",
    "                        cl = 0.6)\n",
    "\n",
    "parser.parse_page()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
